{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import Functions as func\n",
    "import GetPickles\n",
    "import var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model_normal(dim):\n",
    "    \n",
    "    inp_shape = dim\n",
    "    act = 'relu'\n",
    "    drop = .5 \n",
    "    kernal_reg = regularizers.l1(.001)\n",
    "    dil_rate = 2\n",
    "    \n",
    "    \n",
    "    model = Sequential() \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape, \n",
    "                     kernel_regularizer = kernal_reg,\n",
    "                     kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))\n",
    "    #model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),  strides = (2,2)))\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    #model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))\n",
    "    \n",
    "\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (2,2)))\n",
    "\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', name = 'Output_Layer'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = GetPickles.get_samples('normal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10317, 96, 96, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_ellesmere.0\"\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "augment = True \n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience=8)\n",
    "model_checkpoint = ModelCheckpoint('../models/CNN-ModelCheckpointWeights.h5', verbose = 1, save_best_only=True,\n",
    "                                  monitor = 'val_loss')\n",
    "lr_plat = ReduceLROnPlateau(patience = 3, mode = 'min')\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "if var.img_type == 'grey': \n",
    "    dim = (x_train.shape[1], x_train.shape[2], 1)\n",
    "else: \n",
    "    dim = (x_train.shape[1], x_train.shape[2], 3)\n",
    "    \n",
    "normal_model = get_conv_model_normal(dim =dim)\n",
    "\n",
    "if augment: \n",
    "    augmentation =ImageDataGenerator(rotation_range = 15, width_shift_range = .1, height_shift_range = .1, \n",
    "                                                           horizontal_flip = True, fill_mode = 'nearest')\n",
    "    augmentation.fit(x_train)\n",
    "    normal_history = normal_model.fit_generator(augmentation.flow(x_train, y_train, batch_size = batch_size),\n",
    "                epochs = epochs, \n",
    "         callbacks = [early_stopping, model_checkpoint, lr_plat], validation_data = (x_test, y_test), verbose= 1)\n",
    "else: \n",
    "    \n",
    "    normal_history = normal_model.fit(x_train, y_train, batch_size = batch_size,\n",
    "                epochs = epochs, \n",
    "         callbacks = [early_stopping, model_checkpoint, lr_plat], validation_data = (x_test, y_test), verbose= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(normal_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = normal_history.history['loss']\n",
    "train_acc = normal_history.history['acc']\n",
    "test_loss = normal_history.history['val_loss']\n",
    "test_acc = normal_history.history['val_acc']\n",
    "epochs = [i for i in range(1, len(test_acc)+1)]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "ax[0].plot(epochs, train_loss, label = 'Train Loss')\n",
    "ax[0].plot(epochs, test_loss, label = 'Test Loss')\n",
    "ax[0].set_title('Train/Test Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epochs, train_acc, label = 'Train Accuracy')\n",
    "ax[1].plot(epochs, test_acc, label = 'Test Accuracy')\n",
    "ax[1].set_title('Train/Test Accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, f1_score, auc\n",
    "\n",
    "if var.img_type == 'grey': \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 1)\n",
    "else: \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 3)\n",
    "    \n",
    "normal_model = get_conv_model_normal(dim)\n",
    "normal_model.load_weights('../models/CNN-ModelCheckpointWeights.h5') #load the best weights before overfitting\n",
    " \n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,6))\n",
    "#AUC CURVE\n",
    "y_test_prob = normal_model.predict(x_test)\n",
    "\n",
    "y_test_precision, y_test_recall, spec = precision_recall_curve(y_test, y_test_prob)\n",
    "y_test_predict = np.where(y_test_prob >= .5, 1, 0).ravel()\n",
    "y_test_f1= f1_score(y_test, y_test_predict)\n",
    "y_test_auc = auc(y_test_recall, y_test_precision)\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "ax[0].plot(y_test_recall, y_test_precision, marker='.', label='CNN')\n",
    "ax[0].plot([0, 1], [no_skill, no_skill], linestyle='--', label='50/50', color = 'Black')\n",
    "ax[0].set_xlabel('Recall')\n",
    "ax[0].set_ylabel('Precision')\n",
    "ax[0].set_title(f'AUC Curve')\n",
    "ax[0].legend()\n",
    "\n",
    "#ROC CURVE\n",
    "ns_probs = [0 for i in range(len(y_test))]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "y_test_roc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "y_test_fpr, y_test_tpr, threshold = roc_curve(y_test, y_test_prob)\n",
    "ax[1].plot(ns_fpr, ns_tpr, linestyle='--', label='50/50')\n",
    "ax[1].plot(y_test_fpr, y_test_tpr, marker='.', label='CNN')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title(f'ROC Curve')\n",
    "ax[1].legend()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame({'F1 Score': round(y_test_f1, 3), 'AUC': round(y_test_auc, 3), 'ROC':round(y_test_roc, 3)}, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Threshold': threshold, 'FPR': y_test_fpr, 'TPR': y_test_tpr})\n",
    "plt.plot(df.Threshold, df.FPR, label = 'False Positive Rate')\n",
    "plt.plot(df.Threshold, df.TPR, label = 'True Positive Rate')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('FPR/TPR')\n",
    "plt.title('Change in FPR/TPR with Threshold')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "def plot_confusion_matrix(y_test,y_train, y_train_prob, y_test_prob,thresholds, classes,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    fig, ax = plt.subplots(len(thresholds),2, figsize = (10,10))\n",
    "\n",
    "    for idx, thresh in enumerate(thresholds):\n",
    "        y_test_predict = np.where(y_test_prob >= thresh, 1, 0)\n",
    "        y_train_predict = np.where(y_train_prob >= thresh, 1, 0)\n",
    "        train_cm = confusion_matrix(y_train, y_train_predict) \n",
    "        test_cm = confusion_matrix(y_test, y_test_predict)\n",
    "        \n",
    "        #test confusion\n",
    "        ax[idx, 0].imshow(test_cm,  cmap=plt.cm.Blues) \n",
    "\n",
    "        ax[idx, 0].set_title(f'Test: Confusion Matrix | Threshold: {thresh}')\n",
    "        ax[idx, 0].set_ylabel('True label')\n",
    "        ax[idx, 0].set_xlabel('Predicted label')\n",
    "\n",
    "        class_names = classes \n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        ax[idx, 0].set_xticks(tick_marks)\n",
    "        ax[idx,0].set_xticklabels(class_names)\n",
    "        ax[idx, 0].set_yticks(tick_marks)\n",
    "        ax[idx, 0].set_yticklabels(class_names)\n",
    "\n",
    "        th = test_cm.max() / 2. \n",
    "\n",
    "        for i, j in itertools.product(range(test_cm.shape[0]), range(test_cm.shape[1])):\n",
    "                ax[idx, 0].text(j, i, f'{test_cm[i, j]}',# | {int(round(test_cm[i,j]/test_cm.ravel().sum(),5)*100)}%',\n",
    "                         horizontalalignment='center',\n",
    "                         color='white' if test_cm[i, j] > th else 'black')\n",
    "        ax[idx, 0].set_ylim([-.5,1.5])\n",
    "        \n",
    "        #TRAIN CONFUSION\n",
    "        ax[idx, 1].imshow(train_cm,  cmap=plt.cm.Blues) \n",
    "\n",
    "        ax[idx, 1].set_title(f'Train: Confusion Matrix | Threshold: {thresh}')\n",
    "        ax[idx, 1].set_ylabel('True label')\n",
    "        ax[idx, 1].set_xlabel('Predicted label')\n",
    "\n",
    "        class_names = classes \n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        ax[idx, 1].set_xticks(tick_marks)\n",
    "        ax[idx,1].set_xticklabels(class_names)\n",
    "        ax[idx, 1].set_yticks(tick_marks)\n",
    "        ax[idx, 1].set_yticklabels(class_names)\n",
    "\n",
    "\n",
    "        th = train_cm.max() / 2. \n",
    "\n",
    "        for i, j in itertools.product(range(train_cm.shape[0]), range(train_cm.shape[1])):\n",
    "                ax[idx, 1].text(j, i, f'{train_cm[i, j]}',# | {int(round(train_cm[i,j]/train_cm.ravel().sum(),5)*100)}%',\n",
    "                         horizontalalignment='center',\n",
    "                         color='white' if train_cm[i, j] > th else 'black')\n",
    "        ax[idx, 1].set_ylim([-.5,1.5])\n",
    "    plt.tight_layout()\n",
    " \n",
    "    plt.show()\n",
    "if var.img_type == 'grey': \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 1)\n",
    "else: \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 3)\n",
    "        \n",
    "normal_model = get_conv_model_normal(dim)\n",
    "normal_model.load_weights('../models/CNN-ModelCheckpointWeights.h5') #load the best weights before overfitting\n",
    "\n",
    "y_test_prob = normal_model.predict(x_test).ravel()\n",
    "\n",
    "y_train_prob = normal_model.predict(x_train).ravel()\n",
    "\n",
    "plot_confusion_matrix(y_train = y_train, y_test = y_test, y_train_prob = y_train_prob,\n",
    "                      y_test_prob = y_test_prob,classes = ['No Weapon', 'Weapon'], thresholds = [.2, .5,.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Google Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "if var.img_type == 'grey': \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 1)\n",
    "else: \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 3)\n",
    "        \n",
    "normal_model = get_conv_model_normal(dim)\n",
    "normal_model.load_weights('../models/CNN-ModelCheckpointWeights.h5')\n",
    "img = func.get_image_value('../test3.jpg', var.norm_dimension, var.img_type)\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "explanation = explainer.explain_instance(img, normal_model.predict, top_labels = 5, hide_color = 0, \n",
    "                                         num_samples = 1000)\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only = False,\n",
    "                                           num_features = 10, hide_rest = False)\n",
    "plt.imshow(mark_boundaries(temp/2 + .5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if var.img_type == 'grey': \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 1)\n",
    "else: \n",
    "    dim = (var.norm_dimension[0], var.norm_dimension[1], 3)\n",
    "        \n",
    "normal_model = get_conv_model_normal(dim)\n",
    "normal_model.load_weights('../models/CNN-ModelCheckpointWeights.h5')\n",
    "\n",
    "img = cv2.imread('../test3.jpg')\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(img)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "rects = ss.process() \n",
    "\n",
    "windows = []\n",
    "locations = []\n",
    "for x, y, w,h in rects: \n",
    "    startx = x \n",
    "    starty = y \n",
    "    endx = x+w \n",
    "    endy = y+h \n",
    "    roi = img[starty:endy, startx:endx]\n",
    "    roi = cv2.resize(roi, dsize =var.norm_dimension, interpolation = cv2.INTER_CUBIC)\n",
    "    windows.append(roi)\n",
    "    locations.append((startx, starty, endx, endy))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = np.array(windows)\n",
    "\n",
    "predictions = normal_model.predict(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in predictions:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone = img.copy()\n",
    "new_loc = [] \n",
    "new_prob = [] \n",
    "for idx, i in enumerate(predictions): \n",
    "    if i != 1: \n",
    "        continue\n",
    "    startx, starty, endx, endy = locations[idx]\n",
    "    cv2.rectangle(clone, (startx, starty), (endx, endy),  (0,255,0),2)\n",
    "    text = f'Weapon: {i*100}'\n",
    "    cv2.putText(clone, text, (startx, y), cv2.FONT_HERSHEY_SIMPLEX, .45, (0,255,0),2)\n",
    "    new_loc.append(locations[idx])\n",
    "    new_prob.append(i[0])\n",
    "    \n",
    "cv2.imshow('test', clone)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxid = func.non_max_suppression(np.array(new_loc), np.array(new_prob))[0]\n",
    "boxid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone = img.copy()\n",
    "  \n",
    "startx, starty, endx, endy = new_loc[boxid]\n",
    "cv2.rectangle(clone, (startx, starty), (endx, endy),  (0,255,0),2)\n",
    "text = f'Weapon: {i*100}'\n",
    "cv2.putText(clone, text, (startx, y), cv2.FONT_HERSHEY_SIMPLEX, .45, (0,255,0),2)\n",
    "cv2.imshow('test', clone)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone = img.copy()\n",
    "roi = clone[starty:endy, startx:endx]\n",
    "roi = cv2.resize(roi, dsize =var.dimension, interpolation = cv2.INTER_CUBIC)\n",
    "# cv2.imshow('test', roi)\n",
    "# cv2.waitKey(0)\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "explanation = explainer.explain_instance(roi, normal_model.predict, top_labels = 5, hide_color = 0, \n",
    "                                         num_samples = 1000)\n",
    "\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only = False,\n",
    "                                           num_features = 10, hide_rest = False)\n",
    "plt.imshow(mark_boundaries(temp/2 + .5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
